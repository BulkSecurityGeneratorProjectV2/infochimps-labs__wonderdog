h1. Wonderdog

Wonderdog is a bulkloader for Elastic Search.

h2. Requirements

h3. Hadoop cluster setup:

Wonderdog makes use of hadoop to do its bulk loading so you'll need to have a fully functional hadoop cluster lying around. Additionally, each machine in the hadoop cluster will need to have "elasticsearch":http://www.elasticsearch.com/download/ installed in the same location on each machine. Defaults for most everything will want elasticsearch itself to live in @/usr/local/share/elasticsearch@ and the configuration @elasticsearch/config/*@ to actually live in @/etc/elasticsearch@. For simplicity you might set @ES_HOME@ and @ES_CONF@ to point to the right places. As an example setup process (with elasticsearch 0.14.2):

<pre><code>
$: wget http://github.com/downloads/elasticsearch/elasticsearch/elasticsearch-0.14.2.zip
$: unzip elasticsearch-0.14.2.zip
$: sudo mv elasticsearch-0.14.2 /usr/local/share/
$: sudo ln -s /usr/local/share/elasticsearch-0.14.2 /usr/local/share/elasticsearch
$: sudo cp config/* /etc/elasticsearch/
</code></pre>

Then, make all your changes to the configuration files in @/etc/elasticsearch@. See @config/*@ for a set of example configuration files.

h3. ElasticSearch cluster setup:

Follow the same procedure as that for the hadoop cluster setup. Once elasticsearch is installed on all the machines you'll want to start the daemons. It's generally a good idea to have created an elasticsearch user and run the daemons as this user. Then:

<pre><code>
sudo -u elasticsearch $ES_HOME/bin/elasticsearch -Des.config=/etc/elasticsearch/elasticsearch.yml
</code></pre>

should get you started. Do this for every machine in you elasticsearch cluster. Finally, both

@java/bin/estool --host=<elasticsearch_host> status@ and
@java/bin/estool --host=<elasticsearch_host> health@

should indicate a happy elasticsearch cluster.

In addition to the setup for each cluster you'll need to ensure they can talk to each other. This is addressed in the documentation for elasticsearch itself.

h2. Usage

Once you've got a working set up you should be ready to launch your bulkload process. Right now the bulkload only accepts tsv though json and avro support should be possible with straightforward changes (contributions?).

The best way to explain is with an example. Say you've got a tsv file of user records (name,login,email,description) and you want to index all the fields. Assuming you're going to write to an index called @users@ with objects of type @user@ (elasticsearch will create this object automatically the first time you upload one). The workflow is as follows:

* Create the @users@ index:

<pre><code>
bin/estool --host=<elasticsearch_host> --index_name=users create_index
</code></pre>

* Upload the data

<pre><code>
# Will only work if the hadoop elasticsearch processes can discover the running elasticsearch cluster
bin/wonderdog --rm --index_name=users --bulk_size=4096 --object_type=user --field_names=name,login,email,description --id_field=1 /hdfs/path/to/users.tsv /tmp/failed_records/users
</code></pre>

Notice the output path. When the bulk indexing job runs it is possible for index requests to fail for various reasons (too much load, etc). In this case the documents that failed are simply written to the hdfs so they can be retried in a later job.

* Refresh Index

After the bulk load is finished you'll want to refresh the index so your documents will actually be searchable:

<pre><code>
bin/estool --host=<elasticsearch_host> --index_name=users refresh_index
</code></pre>

* Snapshot Index

You'll definitely want to do this after the bulk load finishes so you don't lose any data in case of cluster failure:

<pre><code>
bin/estool --host=<elasticsearch_host> --index_name=users snapshot_index
</code></pre>


* Bump the replicas for the index up to at least one.

<pre><code>
bin/estool --host=<elasticsearch_host> --index_name=users --replicas=1 set_replicas
</code></pre>

This will take a while to finish and the cluster health will show yellow until it does.

* Optimize the index

<pre><code>
bin/estool --host=<elasticsearch_host> --index_name=users optimize_index
</code></pre>

This will also take a while to finish.

h2. Wonderdog Command-line options

* @index_name@ - Index to write data to. It does not have to exist ahead of time
* @object_type@ - Type of object to index. The mapping for this object does not have to exist ahead of time. Fields will be updated dynamically by elasticsearch.
* @field_names@ - A comma separated list of field names describing the tsv record input
* @id_field@ - Index of field to use as object id (counting from 0; default 1), use -1 if there is no id field
* @bulk_size@ - Number of records per bulk request sent to elasticsearch cluster
* @es_home@ - Path to elasticsearch installation, read from the ES_HOME environment variable if it's set
* @es_config@ - Path to elasticsearch config file (@elasticsearch.yml@)
* @rm@ - Remove existing output? (true or leave blank)
* @hadoop_home@ - Path to hadoop installation, read from the HADOOP_HOME environment variable if it's set
* @min_split_size@ - Min split size for maps

h2. Admin

There are a number of convenience commands in @bin/estool@. Enumerating a few:

* Print status of all indices as a json hash to the terminal

<pre><code>
# See everything (tmi)
bin/estool --host=<elasticsearch_host> status

# Just see index level stuff on docs and replicas:
bin/estool --host=<elasticsearch_host> status | grep -C10 number_of_replicas
</code></pre>

* Check cluster health (red,green,yellow,relocated shards, etc)

<pre><code>
bin/estool --host=<elasticsearch_host> health
</code></pre>

* Set replicas for an index

<pre><code>
bin/estool --host=<elasticsearch_host> --index_name=<index_name> --replicas=<num_replicas> set_replicas
</code></pre>

* Optimize an index

<pre><code>
bin/estool --host=<elasticsearch_host> --index_name=<index_name> optimize_index
</code></pre>

* Snapshot an index

<pre><code>
bin/estool --host=<elasticsearch_host> --index_name=<index_name> snapshot_index
</code></pre>

* Delete an index

You'd better be sure.

<pre><code>
bin/estool --host=<elasticsearch_host> --index_name=<index_name> delete_index
</code></pre>


And so on. Most of the common rest api operations have be mapped into estool.

h3. How to choose shards, replicas and cluster size: Rules of Thumb.

sh      = shards
rf      = replication factor. replicas = 0 implies rf = 1, or 1 replica of each shard.

pm      = running data_esnode processes per machine
N       = number of machines

n_cores = number of cpu cores per machine
n_disks = number of disks per machine



* You must have at least as many data_esnodes as 
  Mandatory:  (sh * rf) < (pm * N)


  Shards:     shard size < 10GB


More shards = more parallel writes



curl -XPUT "`cat /etc/motd | grep IP | cuttab 2`:9200/tweet-2010q1/_settings" -d '{ "index":{ "number_of_replicas":1 } }'
